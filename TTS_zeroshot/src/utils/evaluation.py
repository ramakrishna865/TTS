def compute_f1(tp, fp, fn):
    """
    Computes the F1 score given true positives (tp), false positives (fp), and false negatives (fn).
    """
    if tp + fp == 0 or tp + fn == 0:
        return 0.0
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    return 2 * (precision * recall) / (precision + recall)


def calculate_macro_f1(against_f1, favor_f1, neutral_f1):
    """
    Calculates Macro F1 score as the average of Against, Favor, and Neutral F1 scores.
    """
    return (against_f1 + favor_f1 + neutral_f1) / 3


def format_results(metrics):
    """
    Formats the evaluation metrics into a dictionary matching Table 4/Table 5.
    """
    return {
        "Against_Precision": metrics["Against"]["Precision"],
        "Against_Recall": metrics["Against"]["Recall"],
        "Against_F1": metrics["Against"]["F1"],
        "Favor_Precision": metrics["Favor"]["Precision"],
        "Favor_Recall": metrics["Favor"]["Recall"],
        "Favor_F1": metrics["Favor"]["F1"],
        "Neutral_Precision": metrics["Neutral"]["Precision"],
        "Neutral_Recall": metrics["Neutral"]["Recall"],
        "Neutral_F1": metrics["Neutral"]["F1"],
        "Macro_F1": calculate_macro_f1(
            metrics["Against"]["F1"],
            metrics["Favor"]["F1"],
            metrics["Neutral"]["F1"]
        )
    }


def evaluate_model(model, data_loader):
    """
    Evaluates the model and computes metrics including Precision, Recall, and F1 scores.
    """
    # Replace these with the actual metric calculation logic for your data_loader
    against_tp, against_fp, against_fn = 78, 22, 24
    favor_tp, favor_fp, favor_fn = 61, 39, 40
    neutral_tp, neutral_fp, neutral_fn = 53, 47, 46

    # Compute metrics for each class
    metrics = {
        "Against": {
            "Precision": against_tp / (against_tp + against_fp),
            "Recall": against_tp / (against_tp + against_fn),
            "F1": compute_f1(against_tp, against_fp, against_fn),
        },
        "Favor": {
            "Precision": favor_tp / (favor_tp + favor_fp),
            "Recall": favor_tp / (favor_tp + favor_fn),
            "F1": compute_f1(favor_tp, favor_fp, favor_fn),
        },
        "Neutral": {
            "Precision": neutral_tp / (neutral_tp + neutral_fp),
            "Recall": neutral_tp / (neutral_tp + neutral_fn),
            "F1": compute_f1(neutral_tp, neutral_fp, neutral_fn),
        },
    }

    # Format results to match the required output format
    return format_results(metrics)
